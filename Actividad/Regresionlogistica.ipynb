{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freddy-7/TI3002C/blob/main/9_Regresion_Logistica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "38EWRjuebp99"
      },
      "outputs": [],
      "source": [
        "#Cargamos librerias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.special as special\n",
        "from scipy.optimize import curve_fit\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9009 entries, 0 to 9008\n",
            "Data columns (total 79 columns):\n",
            " #   Column                                        Non-Null Count  Dtype  \n",
            "---  ------                                        --------------  -----  \n",
            " 0   id                                            9009 non-null   int64  \n",
            " 1   listing_url                                   9009 non-null   object \n",
            " 2   scrape_id                                     9009 non-null   int64  \n",
            " 3   last_scraped                                  9009 non-null   object \n",
            " 4   source                                        9009 non-null   object \n",
            " 5   name                                          9009 non-null   object \n",
            " 6   description                                   9009 non-null   object \n",
            " 7   neighborhood_overview                         9009 non-null   object \n",
            " 8   picture_url                                   9009 non-null   object \n",
            " 9   host_id                                       9009 non-null   int64  \n",
            " 10  host_url                                      9009 non-null   object \n",
            " 11  host_name                                     9009 non-null   object \n",
            " 12  host_since                                    9009 non-null   object \n",
            " 13  host_location                                 9009 non-null   object \n",
            " 14  host_about                                    9009 non-null   object \n",
            " 15  host_response_time                            9009 non-null   object \n",
            " 16  host_response_rate                            9009 non-null   object \n",
            " 17  host_acceptance_rate                          9009 non-null   object \n",
            " 18  host_is_superhost                             9009 non-null   object \n",
            " 19  host_thumbnail_url                            9009 non-null   object \n",
            " 20  host_picture_url                              9009 non-null   object \n",
            " 21  host_neighbourhood                            9009 non-null   object \n",
            " 22  host_listings_count                           9009 non-null   float64\n",
            " 23  host_total_listings_count                     9009 non-null   float64\n",
            " 24  host_verifications                            9009 non-null   object \n",
            " 25  host_has_profile_pic                          9009 non-null   object \n",
            " 26  host_identity_verified                        9009 non-null   object \n",
            " 27  neighbourhood                                 9009 non-null   object \n",
            " 28  neighbourhood_cleansed                        9009 non-null   object \n",
            " 29  neighbourhood_group_cleansed                  9009 non-null   object \n",
            " 30  latitude                                      9009 non-null   float64\n",
            " 31  longitude                                     9009 non-null   float64\n",
            " 32  property_type                                 9009 non-null   object \n",
            " 33  room_type                                     9009 non-null   object \n",
            " 34  accommodates                                  9009 non-null   float64\n",
            " 35  bathrooms                                     9009 non-null   float64\n",
            " 36  bathrooms_text                                9009 non-null   object \n",
            " 37  bedrooms                                      9009 non-null   float64\n",
            " 38  beds                                          9009 non-null   float64\n",
            " 39  amenities                                     9009 non-null   object \n",
            " 40  price                                         9009 non-null   float64\n",
            " 41  minimum_nights                                9009 non-null   float64\n",
            " 42  maximum_nights                                9009 non-null   float64\n",
            " 43  minimum_minimum_nights                        9009 non-null   float64\n",
            " 44  maximum_minimum_nights                        9009 non-null   float64\n",
            " 45  minimum_maximum_nights                        9009 non-null   int64  \n",
            " 46  maximum_maximum_nights                        9009 non-null   int64  \n",
            " 47  minimum_nights_avg_ntm                        9009 non-null   float64\n",
            " 48  maximum_nights_avg_ntm                        9009 non-null   float64\n",
            " 49  calendar_updated                              9009 non-null   object \n",
            " 50  has_availability                              9009 non-null   object \n",
            " 51  availability_30                               9009 non-null   int64  \n",
            " 52  availability_60                               9009 non-null   int64  \n",
            " 53  availability_90                               9009 non-null   int64  \n",
            " 54  availability_365                              9009 non-null   int64  \n",
            " 55  calendar_last_scraped                         9009 non-null   object \n",
            " 56  number_of_reviews                             9009 non-null   float64\n",
            " 57  number_of_reviews_ltm                         9009 non-null   float64\n",
            " 58  number_of_reviews_l30d                        9009 non-null   float64\n",
            " 59  availability_eoy                              9009 non-null   int64  \n",
            " 60  number_of_reviews_ly                          9009 non-null   float64\n",
            " 61  estimated_occupancy_l365d                     9009 non-null   int64  \n",
            " 62  estimated_revenue_l365d                       9009 non-null   float64\n",
            " 63  first_review                                  9009 non-null   object \n",
            " 64  last_review                                   9009 non-null   object \n",
            " 65  review_scores_rating                          9009 non-null   float64\n",
            " 66  review_scores_accuracy                        9009 non-null   float64\n",
            " 67  review_scores_cleanliness                     9009 non-null   float64\n",
            " 68  review_scores_checkin                         9009 non-null   float64\n",
            " 69  review_scores_communication                   9009 non-null   float64\n",
            " 70  review_scores_location                        9009 non-null   float64\n",
            " 71  review_scores_value                           9009 non-null   float64\n",
            " 72  license                                       9009 non-null   object \n",
            " 73  instant_bookable                              9009 non-null   object \n",
            " 74  calculated_host_listings_count                9009 non-null   float64\n",
            " 75  calculated_host_listings_count_entire_homes   9009 non-null   float64\n",
            " 76  calculated_host_listings_count_private_rooms  9009 non-null   float64\n",
            " 77  calculated_host_listings_count_shared_rooms   9009 non-null   float64\n",
            " 78  reviews_per_month                             9009 non-null   float64\n",
            "dtypes: float64(32), int64(11), object(36)\n",
            "memory usage: 5.4+ MB\n"
          ]
        }
      ],
      "source": [
        "#Convertir archivo filtrado a CSV\n",
        "df = pd.read_csv(\"valencia_trabajo.csv\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "escalar = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df['host_response_rate'] = (\n",
        "    df['host_response_rate']\n",
        "    .astype(str)\n",
        "    .str.replace('%', '', regex=False)\n",
        "    .replace('nan', np.nan)\n",
        "    .astype(float)\n",
        ")\n",
        "\n",
        "cols_bool = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', \n",
        "             'has_availability', 'instant_bookable']\n",
        "\n",
        "df[cols_bool] = df[cols_bool].replace({'t': 1, 'f': 0}).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   host_is_superhost  host_has_profile_pic  host_identity_verified  \\\n",
            "0                  0                     1                       1   \n",
            "1                  0                     1                       1   \n",
            "2                  0                     1                       1   \n",
            "3                  1                     1                       1   \n",
            "4                  0                     1                       1   \n",
            "\n",
            "   has_availability  instant_bookable  \n",
            "0                 1                 0  \n",
            "1                 1                 0  \n",
            "2                 1                 0  \n",
            "3                 1                 1  \n",
            "4                 1                 0  \n"
          ]
        }
      ],
      "source": [
        "cols_bool = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified',\n",
        "             'has_availability', 'instant_bookable']\n",
        "\n",
        "# Reemplazar 't' y 'f' por 1 y 0 solo en esas columnas\n",
        "df[cols_bool] = df[cols_bool].replace({'t': 1, 'f': 0})\n",
        "\n",
        "print(df[cols_bool].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def entrenar_logit_univariada(y, X):\n",
        "    \"\"\"\n",
        "    y: Serie binaria 0/1 (target)\n",
        "    X: Serie numérica (predictor)\n",
        "    \"\"\"\n",
        "    data = pd.concat([y, X], axis=1).dropna()\n",
        "    y_clean = data.iloc[:,0].astype(int)\n",
        "    X_clean = data.iloc[:,1].to_frame()\n",
        "\n",
        "    # Split (estratificado como buena práctica)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_clean, y_clean, test_size=0.2, random_state=None, stratify=y_clean\n",
        "    )\n",
        "\n",
        "    # Escalamiento SOLO a X\n",
        "    escalar = StandardScaler()\n",
        "    X_train = escalar.fit_transform(X_train)\n",
        "    X_test  = escalar.transform(X_test)\n",
        "\n",
        "    # Modelo\n",
        "    modelo = LogisticRegression(max_iter=1000)\n",
        "    modelo.fit(X_train, y_train)\n",
        "\n",
        "    # Predicción\n",
        "    y_pred = modelo.predict(X_test)\n",
        "\n",
        "    # Métricas\n",
        "    m_conf = confusion_matrix(y_test, y_pred)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec  = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1   = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "    print(\"Matriz de Confusión:\")\n",
        "    print(m_conf)\n",
        "    print(\"\\nPrecisión (accuracy):\", acc)\n",
        "    print(\"Precisión (precision):\", prec)\n",
        "    print(\"Sensibilidad (recall):\", rec)\n",
        "    print(\"Puntaje F1:\", f1)\n",
        "\n",
        "    return modelo, (acc, prec, rec, f1)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de Confusión:\n",
            "[[905   0]\n",
            " [357   0]]\n",
            "\n",
            "Precisión (accuracy): 0.7171156893819335\n",
            "Precisión (precision): 0.0\n",
            "Sensibilidad (recall): 0.0\n",
            "Puntaje F1: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Definir X y y antes de hacer el split\n",
        "X = df[[\"host_response_rate\"]]  \n",
        "y = df[\"host_is_superhost\"]    \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
        "modelo_1, metrics_1 = entrenar_logit_univariada(y_train, X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de Confusión:\n",
            "[[ 564 1381]\n",
            " [  52  706]]\n",
            "\n",
            "Precisión del modelo:\n",
            "0.33828461907043605\n",
            "\n",
            "Exactitud del modelo:\n",
            "0.46984831668516464\n",
            "\n",
            "Sensibilidad del modelo:\n",
            "0.9313984168865436\n"
          ]
        }
      ],
      "source": [
        "algoritmo_Pond2 = LogisticRegression(class_weight='balanced')\n",
        "algoritmo_Pond2.fit(X_train, y_train)\n",
        "\n",
        "y_pred_pond2 = algoritmo_Pond2.predict(X_test) \n",
        "y_pred_pond2\n",
        "\n",
        "matriz = confusion_matrix(y_test, y_pred_pond2)\n",
        "print('Matriz de Confusión:')\n",
        "print(matriz)\n",
        "print()\n",
        "\n",
        "precision = precision_score(y_test, y_pred_pond2, average=\"binary\", pos_label=1)\n",
        "print('Precisión del modelo:')\n",
        "print(precision)\n",
        "print()\n",
        "\n",
        "exactitud = accuracy_score(y_test, y_pred_pond2)\n",
        "print('Exactitud del modelo:')\n",
        "print(exactitud)\n",
        "print()\n",
        "\n",
        "sensibilidad = recall_score(y_test, y_pred_pond2, average=\"binary\", pos_label=1)\n",
        "print('Sensibilidad del modelo:')\n",
        "print(sensibilidad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de Confusión:\n",
            "[[   0   58]\n",
            " [   0 1204]]\n",
            "\n",
            "Precisión (accuracy): 0.9540412044374009\n",
            "Precisión (precision): 0.9540412044374009\n",
            "Sensibilidad (recall): 1.0\n",
            "Puntaje F1: 0.9764801297648013\n"
          ]
        }
      ],
      "source": [
        "y = df[\"host_has_profile_pic\"]\n",
        "X = df[\"review_scores_value\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
        "modelo_2, metrics_2 = entrenar_logit_univariada(y_train, X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de Confusión:\n",
            "[[  26   78]\n",
            " [1038 1561]]\n",
            "\n",
            "Precisión del modelo:\n",
            "0.9524100061012812\n",
            "\n",
            "Exactitud del modelo:\n",
            "0.5871254162042175\n",
            "\n",
            "Sensibilidad del modelo:\n",
            "0.6006156213928434\n"
          ]
        }
      ],
      "source": [
        "algoritmo_Pond2 = LogisticRegression(class_weight='balanced')\n",
        "\n",
        "# Asegura que X_train y X_test sean DataFrames\n",
        "X_train = X_train.to_frame()\n",
        "X_test = X_test.to_frame()\n",
        "\n",
        "algoritmo_Pond2.fit(X_train, y_train)\n",
        "\n",
        "y_pred_pond2 = algoritmo_Pond2.predict(X_test)\n",
        "y_pred_pond2\n",
        "\n",
        "matriz = confusion_matrix(y_test, y_pred_pond2)\n",
        "print('Matriz de Confusión:')\n",
        "print(matriz)\n",
        "print()\n",
        "\n",
        "precision = precision_score(y_test, y_pred_pond2, average=\"binary\", pos_label=1)\n",
        "print('Precisión del modelo:')\n",
        "print(precision)\n",
        "print()\n",
        "\n",
        "exactitud = accuracy_score(y_test, y_pred_pond2)\n",
        "print('Exactitud del modelo:')\n",
        "print(exactitud)\n",
        "print()\n",
        "\n",
        "sensibilidad = recall_score(y_test, y_pred_pond2, average=\"binary\", pos_label=1)\n",
        "print('Sensibilidad del modelo:')\n",
        "print(sensibilidad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de Confusión:\n",
            "[[   0   21]\n",
            " [   0 1781]]\n",
            "\n",
            "Precisión (accuracy): 0.98834628190899\n",
            "Precisión (precision): 0.98834628190899\n",
            "Sensibilidad (recall): 1.0\n",
            "Puntaje F1: 0.994138989673458\n"
          ]
        }
      ],
      "source": [
        "y = df[\"has_availability\"]\n",
        "X = df[\"availability_365\"]\n",
        "modelo, metrics = entrenar_logit_univariada(y, X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de Confusión:\n",
            "[[  26   78]\n",
            " [1038 1561]]\n",
            "\n",
            "Precisión del modelo:\n",
            "0.9524100061012812\n",
            "\n",
            "Exactitud del modelo:\n",
            "0.5871254162042175\n",
            "\n",
            "Sensibilidad del modelo:\n",
            "0.6006156213928434\n"
          ]
        }
      ],
      "source": [
        "algoritmo_Pond2 = LogisticRegression(class_weight='balanced')\n",
        "algoritmo_Pond2.fit(X_train, y_train)\n",
        "\n",
        "y_pred_pond2 = algoritmo_Pond2.predict(X_test)\n",
        "y_pred_pond2\n",
        "\n",
        "matriz = confusion_matrix(y_test, y_pred_pond2)\n",
        "print('Matriz de Confusión:')\n",
        "print(matriz)\n",
        "print()\n",
        "\n",
        "precision = precision_score(y_test, y_pred_pond2, average=\"binary\", pos_label=1)\n",
        "print('Precisión del modelo:')\n",
        "print(precision)\n",
        "print()\n",
        "\n",
        "exactitud = accuracy_score(y_test, y_pred_pond2)\n",
        "print('Exactitud del modelo:')\n",
        "print(exactitud)\n",
        "print()\n",
        "\n",
        "sensibilidad = recall_score(y_test, y_pred_pond2, average=\"binary\", pos_label=1)\n",
        "print('Sensibilidad del modelo:')\n",
        "print(sensibilidad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Entire home/apt', 'Hotel room', 'Private room', 'Shared room'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unico = np.unique(df['room_type'])\n",
        "unico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['room_type']= df['room_type'].replace(['Hotel room', 'Private room', 'Shared room'], \"Not Entire home/apt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de Confusión:\n",
            "[[1806  127]\n",
            " [ 176  594]]\n",
            "\n",
            "Precisión del modelo:\n",
            "0.9112008072653885\n",
            "\n",
            "Exactitud del modelo:\n",
            "0.8879023307436182\n",
            "\n",
            "Sensibilidad del modelo:\n",
            "0.934299017071909\n",
            "\n",
            "Puntaje F1 del modelo:\n",
            "0.9226053639846743\n"
          ]
        }
      ],
      "source": [
        "Vars_Indep= df[['price', 'accommodates', 'bedrooms']]\n",
        "Var_Dep= df['room_type']\n",
        "\n",
        "X= Vars_Indep\n",
        "y= Var_Dep\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state =None)\n",
        "\n",
        "escalar = StandardScaler()\n",
        "\n",
        "X_train = escalar.fit_transform(X_train)\n",
        "X_test = escalar.transform(X_test)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "algoritmo = LogisticRegression()\n",
        "\n",
        "algoritmo.fit(X_train, y_train)\n",
        "\n",
        "y_pred = algoritmo.predict(X_test) \n",
        "y_pred\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "matriz = confusion_matrix(y_test, y_pred)\n",
        "print('Matriz de Confusión:')\n",
        "print(matriz)\n",
        "print()\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average=\"binary\", pos_label= \"Entire home/apt\")\n",
        "print('Precisión del modelo:')\n",
        "print(precision)\n",
        "print()\n",
        "\n",
        "exactitud = accuracy_score(y_test, y_pred)\n",
        "print('Exactitud del modelo:')\n",
        "print(exactitud)\n",
        "print()\n",
        "\n",
        "sensibilidad = recall_score(y_test, y_pred, average=\"binary\", pos_label=\"Entire home/apt\")\n",
        "print('Sensibilidad del modelo:')\n",
        "print(sensibilidad)\n",
        "print()\n",
        "\n",
        "puntajef1 = f1_score(y_test, y_pred, average=\"binary\", pos_label=\"Entire home/apt\")\n",
        "print('Puntaje F1 del modelo:')\n",
        "print(puntajef1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  0.,   3.,   6.,   7.,   8.,   9.,  10.,  11.,  13.,  14.,  17.,\n",
              "        18.,  20.,  24.,  25.,  27.,  30.,  31.,  33.,  36.,  39.,  40.,\n",
              "        43.,  44.,  46.,  48.,  50.,  53.,  54.,  56.,  60.,  62.,  63.,\n",
              "        64.,  67.,  68.,  69.,  70.,  71.,  73.,  75.,  77.,  78.,  79.,\n",
              "        80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,  90.,\n",
              "        91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,  99., 100.])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unico = np.unique(df['host_response_rate'])\n",
        "unico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de confusión:\n",
            " [[ 832   93]\n",
            " [1071  707]]\n",
            "\n",
            "Exactitud: 0.5693673695893452\n",
            "\n",
            "Precisión: 0.88375\n",
            "\n",
            "Sensibilidad: 0.39763779527559057\n",
            "\n",
            "F1: 0.5484871993793639\n"
          ]
        }
      ],
      "source": [
        "Vars_Indep = df[['host_is_superhost','review_scores_value','review_scores_rating']].astype(float)\n",
        "\n",
        "umbral = df['host_response_rate'].median()\n",
        "Var_Dep_cls = (df['host_response_rate'] >= umbral).astype(int)  # 1=Alto, 0=Bajo\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Vars_Indep, Var_Dep_cls, test_size=0.3, random_state=None, stratify=Var_Dep_cls)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "clf = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "matriz = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matriz de confusión:\\n\", matriz)\n",
        "print()\n",
        "print(\"Exactitud:\", accuracy_score(y_test, y_pred))\n",
        "print()\n",
        "print(\"Precisión:\", precision_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"Sensibilidad:\", recall_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"F1:\", f1_score(y_test, y_pred, pos_label=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['0', '0%', '10%', '100%', '11%', '13%', '14%', '15%', '16%', '17%',\n",
              "       '18%', '19%', '2%', '20%', '21%', '22%', '23%', '24%', '25%',\n",
              "       '26%', '27%', '29%', '30%', '31%', '33%', '34%', '35%', '36%',\n",
              "       '37%', '38%', '39%', '4%', '40%', '41%', '42%', '43%', '44%',\n",
              "       '45%', '46%', '47%', '48%', '49%', '5%', '50%', '51%', '52%',\n",
              "       '53%', '54%', '55%', '56%', '57%', '58%', '59%', '6%', '60%',\n",
              "       '61%', '62%', '63%', '64%', '65%', '66%', '67%', '68%', '69%',\n",
              "       '7%', '70%', '71%', '72%', '73%', '74%', '75%', '76%', '77%',\n",
              "       '78%', '79%', '8%', '80%', '81%', '82%', '83%', '84%', '85%',\n",
              "       '86%', '87%', '88%', '89%', '9%', '90%', '91%', '92%', '93%',\n",
              "       '94%', '95%', '96%', '97%', '98%', '99%'], dtype=object)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unico = np.unique(df['host_acceptance_rate'])\n",
        "unico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['host_acceptance_rate'] = (\n",
        "    df['host_acceptance_rate']\n",
        "    .astype(str)                    # asegura que todos sean cadenas\n",
        "    .str.replace('%', '', regex=False)  # elimina el símbolo %\n",
        "    .replace('nan', np.nan)         # convierte el texto 'nan' a valor NaN\n",
        "    .astype(float)                  # convierte a float\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de confusión:\n",
            " [[823 524]\n",
            " [626 730]]\n",
            "\n",
            "Exactitud: 0.5745467998520163\n",
            "\n",
            "Precisión: 0.5821371610845295\n",
            "\n",
            "Sensibilidad: 0.5383480825958702\n",
            "\n",
            "F1: 0.5593869731800766\n"
          ]
        }
      ],
      "source": [
        "Vars_Indep = df[['host_is_superhost','review_scores_value','review_scores_rating']].astype(float)\n",
        "\n",
        "umbral = df['host_acceptance_rate'].median()\n",
        "Var_Dep_cls = (df['host_acceptance_rate'] >= umbral).astype(int)  # 1=Alto, 0=Bajo\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Vars_Indep, Var_Dep_cls, test_size=0.3, random_state=None, stratify=Var_Dep_cls)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "clf = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "matriz = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matriz de confusión:\\n\", matriz)\n",
        "print()\n",
        "print(\"Exactitud:\", accuracy_score(y_test, y_pred))\n",
        "print()\n",
        "print(\"Precisión:\", precision_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"Sensibilidad:\", recall_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"F1:\", f1_score(y_test, y_pred, pos_label=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de confusión:\n",
            " [[ 848  348]\n",
            " [ 395 1112]]\n",
            "\n",
            "Exactitud: 0.7251202367739549\n",
            "\n",
            "Precisión: 0.7616438356164383\n",
            "\n",
            "Sensibilidad: 0.7378898473788985\n",
            "\n",
            "F1: 0.7495786990225818\n"
          ]
        }
      ],
      "source": [
        "Vars_Indep = df[['accommodates','bathrooms','bedrooms']].astype(float)\n",
        "\n",
        "umbral = df['price'].median()\n",
        "Var_Dep_cls = (df['price'] >= umbral).astype(int)  # 1=Alto, 0=Bajo\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Vars_Indep, Var_Dep_cls, test_size=0.3, random_state=None, stratify=Var_Dep_cls)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "clf = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "matriz = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matriz de confusión:\\n\", matriz)\n",
        "print()\n",
        "print(\"Exactitud:\", accuracy_score(y_test, y_pred))\n",
        "print()\n",
        "print(\"Precisión:\", precision_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"Sensibilidad:\", recall_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"F1:\", f1_score(y_test, y_pred, pos_label=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de confusión:\n",
            " [[ 839  286]\n",
            " [ 139 1439]]\n",
            "\n",
            "Exactitud: 0.8427672955974843\n",
            "\n",
            "Precisión: 0.8342028985507246\n",
            "\n",
            "Sensibilidad: 0.9119138149556401\n",
            "\n",
            "F1: 0.8713290947623372\n"
          ]
        }
      ],
      "source": [
        "Vars_Indep = df[['review_scores_cleanliness','review_scores_checkin','review_scores_value']].astype(float)\n",
        "\n",
        "umbral = df['review_scores_rating'].median()\n",
        "Var_Dep_cls = (df['review_scores_rating'] >= umbral).astype(int)  # 1=Alto, 0=Bajo\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Vars_Indep, Var_Dep_cls, test_size=0.3, random_state=None, stratify=Var_Dep_cls)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "clf = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "matriz = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matriz de confusión:\\n\", matriz)\n",
        "print()\n",
        "print(\"Exactitud:\", accuracy_score(y_test, y_pred))\n",
        "print()\n",
        "print(\"Precisión:\", precision_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"Sensibilidad:\", recall_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"F1:\", f1_score(y_test, y_pred, pos_label=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de confusión:\n",
            " [[ 647  210]\n",
            " [ 238 1608]]\n",
            "\n",
            "Exactitud: 0.8342582315945246\n",
            "\n",
            "Precisión: 0.8844884488448845\n",
            "\n",
            "Sensibilidad: 0.8710725893824486\n",
            "\n",
            "F1: 0.8777292576419214\n"
          ]
        }
      ],
      "source": [
        "Vars_Indep = df[['review_scores_cleanliness','review_scores_checkin','review_scores_rating']].astype(float)\n",
        "\n",
        "umbral = df['review_scores_value'].median()\n",
        "Var_Dep_cls = (df['review_scores_value'] >= umbral).astype(int)  # 1=Alto, 0=Bajo\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Vars_Indep, Var_Dep_cls, test_size=0.3, random_state=None, stratify=Var_Dep_cls)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "clf = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "matriz = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matriz de confusión:\\n\", matriz)\n",
        "print()\n",
        "print(\"Exactitud:\", accuracy_score(y_test, y_pred))\n",
        "print()\n",
        "print(\"Precisión:\", precision_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"Sensibilidad:\", recall_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"F1:\", f1_score(y_test, y_pred, pos_label=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de confusión:\n",
            " [[1110  199]\n",
            " [ 337 1057]]\n",
            "\n",
            "Exactitud: 0.8017018128005919\n",
            "\n",
            "Precisión: 0.8415605095541401\n",
            "\n",
            "Sensibilidad: 0.7582496413199427\n",
            "\n",
            "F1: 0.7977358490566038\n"
          ]
        }
      ],
      "source": [
        "Vars_Indep = df[['bedrooms','bathrooms','price']].astype(float)\n",
        "\n",
        "umbral = df['accommodates'].median()\n",
        "Var_Dep_cls = (df['accommodates'] >= umbral).astype(int)  # 1=Alto, 0=Bajo\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Vars_Indep, Var_Dep_cls, test_size=0.3, random_state=None, stratify=Var_Dep_cls)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "clf = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "matriz = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matriz de confusión:\\n\", matriz)\n",
        "print()\n",
        "print(\"Exactitud:\", accuracy_score(y_test, y_pred))\n",
        "print()\n",
        "print(\"Precisión:\", precision_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"Sensibilidad:\", recall_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"F1:\", f1_score(y_test, y_pred, pos_label=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de confusión:\n",
            " [[  94   50]\n",
            " [1251 1308]]\n",
            "\n",
            "Exactitud: 0.5186829448760636\n",
            "\n",
            "Precisión: 0.9631811487481591\n",
            "\n",
            "Sensibilidad: 0.511137162954279\n",
            "\n",
            "F1: 0.6678580546336482\n"
          ]
        }
      ],
      "source": [
        "Vars_Indep = df[['accommodates','bathrooms','price']].astype(float)\n",
        "\n",
        "umbral = df['bedrooms'].median()\n",
        "Var_Dep_cls = (df['bedrooms'] >= umbral).astype(int)  # 1=Alto, 0=Bajo\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Vars_Indep, Var_Dep_cls, test_size=0.3, random_state=None, stratify=Var_Dep_cls)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "clf = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "matriz = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matriz de confusión:\\n\", matriz)\n",
        "print()\n",
        "print(\"Exactitud:\", accuracy_score(y_test, y_pred))\n",
        "print()\n",
        "print(\"Precisión:\", precision_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"Sensibilidad:\", recall_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"F1:\", f1_score(y_test, y_pred, pos_label=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "count    9009.0\n",
            "mean        1.0\n",
            "std         0.0\n",
            "min         1.0\n",
            "25%         1.0\n",
            "50%         1.0\n",
            "75%         1.0\n",
            "max         1.0\n",
            "Name: bathrooms, dtype: float64\n",
            "0\n",
            "[1.]\n"
          ]
        }
      ],
      "source": [
        "umbral = df['bathrooms'].quantile(0.5)  # o 0.6, 0.4 según convenga\n",
        "Var_Dep_cls = (df['bathrooms'] > umbral).astype(int)\n",
        "print(df['bathrooms'].describe())\n",
        "print(df['bathrooms'].isna().sum())\n",
        "print(df['bathrooms'].unique())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['bathrooms'] = pd.to_numeric(df['bathrooms'], errors='coerce')\n",
        "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bathrooms\n",
            "1.0    9009\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Resumen estadístico:\n",
            " count    9009.0\n",
            "mean        1.0\n",
            "std         0.0\n",
            "min         1.0\n",
            "25%         1.0\n",
            "50%         1.0\n",
            "75%         1.0\n",
            "max         1.0\n",
            "Name: bathrooms, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "print(df['bathrooms'].value_counts())\n",
        "print(\"\\nResumen estadístico:\\n\", df['bathrooms'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Conteo de clases:\n",
            "bathrooms\n",
            "0    9009\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Modelo\u001b[39;00m\n\u001b[32m     32\u001b[39m clf = LogisticRegression(class_weight=\u001b[33m'\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m'\u001b[39m, max_iter=\u001b[32m1000\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Predicción\u001b[39;00m\n\u001b[32m     36\u001b[39m y_pred = clf.predict(X_test)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hecto\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hecto\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1335\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1333\u001b[39m classes_ = \u001b[38;5;28mself\u001b[39m.classes_\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_classes < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1335\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1336\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis solver needs samples of at least 2 classes\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1337\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m in the data, but the data contains only one\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1338\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % classes_[\u001b[32m0\u001b[39m]\n\u001b[32m   1339\u001b[39m     )\n\u001b[32m   1341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.classes_) == \u001b[32m2\u001b[39m:\n\u001b[32m   1342\u001b[39m     n_classes = \u001b[32m1\u001b[39m\n",
            "\u001b[31mValueError\u001b[39m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# --- Limpieza ---\n",
        "df['bathrooms'] = pd.to_numeric(df['bathrooms'], errors='coerce')\n",
        "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
        "\n",
        "Vars_Indep = df[['accommodates','bedrooms','price']].astype(float)\n",
        "\n",
        "# Definir umbral (usa la mediana)\n",
        "umbral = df['bathrooms'].median()\n",
        "Var_Dep_cls = (df['bathrooms'] > umbral).astype(int)  # > en lugar de >=\n",
        "\n",
        "print(\"\\nConteo de clases:\")\n",
        "print(Var_Dep_cls.value_counts())\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    Vars_Indep, Var_Dep_cls, test_size=0.3, random_state=42, stratify=Var_Dep_cls\n",
        ")\n",
        "\n",
        "# Escalamiento\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# Modelo\n",
        "clf = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predicción\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Métricas\n",
        "matriz = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nMatriz de confusión:\\n\", matriz)\n",
        "print(\"Exactitud:\", round(accuracy_score(y_test, y_pred), 3))\n",
        "print(\"Precisión:\", round(precision_score(y_test, y_pred), 3))\n",
        "print(\"Sensibilidad:\", round(recall_score(y_test, y_pred), 3))\n",
        "print(\"F1:\", round(f1_score(y_test, y_pred), 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de confusión:\n",
            " [[ 851  157]\n",
            " [ 542 1153]]\n",
            "\n",
            "Exactitud: 0.7413984461709212\n",
            "\n",
            "Precisión: 0.8801526717557252\n",
            "\n",
            "Sensibilidad: 0.68023598820059\n",
            "\n",
            "F1: 0.76738768718802\n"
          ]
        }
      ],
      "source": [
        "Vars_Indep = df[['bathrooms','bedrooms','price']].astype(float)\n",
        "\n",
        "umbral = df['beds'].median()\n",
        "Var_Dep_cls = (df['beds'] >= umbral).astype(int)  # 1=Alto, 0=Bajo\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Vars_Indep, Var_Dep_cls, test_size=0.3, random_state=None, stratify=Var_Dep_cls)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "clf = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "matriz = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matriz de confusión:\\n\", matriz)\n",
        "print()\n",
        "print(\"Exactitud:\", accuracy_score(y_test, y_pred))\n",
        "print()\n",
        "print(\"Precisión:\", precision_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"Sensibilidad:\", recall_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"F1:\", f1_score(y_test, y_pred, pos_label=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de confusión:\n",
            " [[ 783  345]\n",
            " [ 178 1397]]\n",
            "\n",
            "Exactitud: 0.8065112837587866\n",
            "\n",
            "Precisión: 0.8019517795637199\n",
            "\n",
            "Sensibilidad: 0.886984126984127\n",
            "\n",
            "F1: 0.8423274042809767\n"
          ]
        }
      ],
      "source": [
        "Vars_Indep = df[['review_scores_value','review_scores_checkin','review_scores_rating']].astype(float)\n",
        "\n",
        "umbral = df['review_scores_cleanliness'].median()\n",
        "Var_Dep_cls = (df['review_scores_cleanliness'] >= umbral).astype(int)  # 1=Alto, 0=Bajo\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Vars_Indep, Var_Dep_cls, test_size=0.3, random_state=None, stratify=Var_Dep_cls)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "clf = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "matriz = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matriz de confusión:\\n\", matriz)\n",
        "print()\n",
        "print(\"Exactitud:\", accuracy_score(y_test, y_pred))\n",
        "print()\n",
        "print(\"Precisión:\", precision_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"Sensibilidad:\", recall_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"F1:\", f1_score(y_test, y_pred, pos_label=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de confusión:\n",
            " [[ 791  337]\n",
            " [ 201 1374]]\n",
            "\n",
            "Exactitud: 0.8009618941916389\n",
            "\n",
            "Precisión: 0.8030391583869082\n",
            "\n",
            "Sensibilidad: 0.8723809523809524\n",
            "\n",
            "F1: 0.8362751065124772\n"
          ]
        }
      ],
      "source": [
        "Vars_Indep = df[['review_scores_value','review_scores_checkin','review_scores_rating']].astype(float)\n",
        "\n",
        "umbral = df['review_scores_cleanliness'].median()\n",
        "Var_Dep_cls = (df['review_scores_cleanliness'] >= umbral).astype(int)  # 1=Alto, 0=Bajo\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Vars_Indep, Var_Dep_cls, test_size=0.3, random_state=None, stratify=Var_Dep_cls)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "clf = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "matriz = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matriz de confusión:\\n\", matriz)\n",
        "print()\n",
        "print(\"Exactitud:\", accuracy_score(y_test, y_pred))\n",
        "print()\n",
        "print(\"Precisión:\", precision_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"Sensibilidad:\", recall_score(y_test, y_pred, pos_label=1))\n",
        "print()\n",
        "print(\"F1:\", f1_score(y_test, y_pred, pos_label=1))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNKtew5CuhPLFGDGfhs25px",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
